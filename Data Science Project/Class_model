import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder


# Load & Clean Data
url = r"D:\Zeyad Folder\Zeyad Uni\Years\Year 3\Term 2\Data Science Methodology\Project\master-5.csv"
data = pd.read_csv(url)

# Select features and target
features = [
    'Daily Social Media Usage(hours)',
    'Number of Social Media Platforms',
    'Frequency of Posts',
    'Frequency of Checking Notifications',
    'Self Reported Addiction Score',
    'Social Media Fatigue Score'
]
target = 'Mental Health Status'

# Drop missing values
data_clean = data[features + [target]].dropna()

# Convert categorical features to numeric to work with the scaler
label_enc = LabelEncoder()
for col in ['Frequency of Posts', 'Frequency of Checking Notifications']:
    data_clean[col] = label_enc.fit_transform(data_clean[col])

# Define X and y
X = data_clean[features]
y_encoded = label_enc.fit_transform(data_clean[target]) #since we have categorical we must encode

# Normalize Data (for kNN) 
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)

# Naive Bayes 
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
y_pred_nb = nb_model.predict(X_test)

# k-Nearest Neighbors 
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)

# Evaluation 
print(" Naive Bayes Report")
print("Accuracy:", accuracy_score(y_test, y_pred_nb))
print(classification_report(y_test, y_pred_nb))

print("\n k-Nearest Neighbors Report")
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print(classification_report(y_test, y_pred_knn))

# Visualization 
plt.figure(figsize=(12, 5))

# Naive Bayes Confusion Matrix
plt.subplot(1, 2, 1)
sns.heatmap(confusion_matrix(y_test, y_pred_nb), annot=True, fmt='d', cmap='Blues')
plt.title("Naive Bayes Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")

# kNN Confusion Matrix
plt.subplot(1, 2, 2)
sns.heatmap(confusion_matrix(y_test, y_pred_knn), annot=True, fmt='d', cmap='Greens')
plt.title("kNN Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")

print("Original rows:", data.shape[0])
print("After cleaning:", data_clean.shape[0])
print("Test set size:", X_test.shape[0])
print("Test set class distribution:\n", pd.Series(y_test).value_counts())
print(label_enc.classes_)

plt.tight_layout()
plt.show()
